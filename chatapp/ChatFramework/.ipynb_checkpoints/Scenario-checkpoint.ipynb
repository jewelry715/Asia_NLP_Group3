{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Json파일 로드와 저장을 위한 라이브러리\n",
    "import json\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "#자연어 토큰화를 위한 라이브러리\n",
    "from konlpy.tag import Okt\n",
    "twitter = Okt()\n",
    "\n",
    "#데이터 로드와 모델 사용을 위한 라이브러리\n",
    "from keras.models import Sequential\n",
    "import keras\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#코사인 유사도 계산을 위한 통계 라이브러리\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "#웹툰 데이터 다운로드\n",
    "webtoon_data = np.load('data/webtoon/web_toon_data.npy',allow_pickle='TRUE').item()\n",
    "\n",
    "opening_model = keras.models.load_model('data/webtoon/opening_conv.h5')\n",
    "opening_model_data = np.load('data/webtoon/opening_model_data.npy',allow_pickle='TRUE').item()\n",
    "open_classes = np.load('data/webtoon/open_classes.npy',allow_pickle='TRUE').item()['file']\n",
    "open_words = np.load('data/webtoon/open_words.npy',allow_pickle='TRUE').item()['file']\n",
    "\n",
    "gerne_model = keras.models.load_model('data/webtoon/gerne_conv.h5')\n",
    "gerne_model_data = np.load('data/webtoon/gerne_model_data.npy',allow_pickle='TRUE').item()\n",
    "gerne_classes = np.load('data/webtoon/gerne_classes.npy',allow_pickle='TRUE').item()['file']\n",
    "gerne_words = np.load('data/webtoon/gerne_words.npy',allow_pickle='TRUE').item()['file']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_word = ['웹툰','추천','내용','이야기','전개']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "for work in webtoon_data:\n",
    "    title = webtoon_data[work]['title']\n",
    "    category = webtoon_data[work]['category']\n",
    "    summary = webtoon_data[work]['summary']\n",
    "    gerne = webtoon_data[work]['gerne']\n",
    "    data.append({\n",
    "        'title':title,\n",
    "        'category':category,\n",
    "        'gerne':gerne,\n",
    "        'summary':category[0] +' '+ category[1] +' '+ summary\n",
    "    })\n",
    "data = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_up_sentence(sentence,stop_word):\n",
    "    # tokenize the pattern\n",
    "#     sentence_words = nltk.word_tokenize(sentence)\n",
    "    pos_result = twitter.pos(sentence, norm=True, stem=True)\n",
    "    sentence_words = [lex for lex, pos in pos_result if lex not in stop_word]\n",
    "    # stem each word\n",
    "#     sentence_words = [stemmer.stem(word.lower()) for word in sentence_words]\n",
    "    return sentence_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bow(sentence, words, show_details=False, stop_word = []):\n",
    "    # tokenize the pattern\n",
    "    sentence_words = clean_up_sentence(sentence,stop_word)\n",
    "    print(sentence_words)\n",
    "    # bag of words\n",
    "    bag = [0]*len(words)  \n",
    "    for s in sentence_words:\n",
    "        for i,w in enumerate(words):\n",
    "            if w == s: \n",
    "                bag[i] = 1\n",
    "                if show_details:\n",
    "                    print (\"found in bag: %s\" % w)\n",
    "\n",
    "    return(np.array(bag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_class(sentense,model,data_len,classes,words,stop_word=[]):\n",
    "    p = bow(sentense, words, stop_word=stop_word)\n",
    "    p = np.array(p)\n",
    "    p = p.reshape(1,data_len)\n",
    "    predict_value = model.predict(p)\n",
    "    class_value = classes[np.argmax(predict_value[0])]\n",
    "    return class_value, predict_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_user_sentence(user_text,answer):\n",
    "    \"\"\"사용자가 원하는 내용을 받아서 데이터셋에 추가한 후 추가한 데이터셋 반환\"\"\"\n",
    "    if answer[0] == answer[1]:\n",
    "        newtf = data['gerne'] == answer[0]\n",
    "        user_data= data[newtf]\n",
    "    else :\n",
    "        newtf = data['gerne'] == answer[0]\n",
    "        newtf2 = data['gerne'] == answer[1]\n",
    "        user_data= data[newtf].append(data[newtf2])\n",
    "        \n",
    "    user_data = user_data.append({'title':'UserData','category':'','gerne':'','summary':user_text},ignore_index=True)\n",
    "    return user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(data):\n",
    "    \"\"\"데이터셋을 받아서 코사인 유사도와 단어 인덱스 사전 반환\"\"\"\n",
    "    tfidf = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf.fit_transform(data['summary'])\n",
    "    print('{}개의 데이터셋과 {}개의 단어 구성'.format(tfidf_matrix.shape[0],tfidf_matrix.shape[1]))\n",
    "    \n",
    "    cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)\n",
    "    indices = pd.Series(data.index, index=data['title']).drop_duplicates()\n",
    "    \n",
    "    return cosine_sim, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_recommendations(data, indices, cosine_sim, title=\"UserData\"):\n",
    "    # 선택한 영화의 타이틀로부터 해당되는 인덱스를 받아옵니다. 이제 선택한 영화를 가지고 연산할 수 있습니다.\n",
    "    idx = indices[title]\n",
    "\n",
    "    # 모든 영화에 대해서 해당 영화와의 유사도를 구합니다.\n",
    "    sim_scores = list(enumerate(cosine_sim[idx]))\n",
    "\n",
    "    # 유사도에 따라 영화들을 정렬합니다.\n",
    "    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # 가장 유사한 10개의 영화를 받아옵니다.\n",
    "    sim_scores = sim_scores[1:11]\n",
    "\n",
    "    # 가장 유사한 10개의 영화의 인덱스를 받아옵니다.\n",
    "    movie_indices = [i[0] for i in sim_scores]\n",
    "\n",
    "    # 가장 유사한 10개의 영화의 제목을 리턴합니다.\n",
    "    return data['title'].iloc[movie_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_scenario():\n",
    "    context = ''\n",
    "    gerne = []\n",
    "    \n",
    "    user_answer = input('어떤 콘텐츠를 추천해 드릴까요?')\n",
    "    answer, _ = predict_class(user_answer,opening_model, opening_model_data['words'], open_classes, open_words)\n",
    "    context += ' '+user_answer\n",
    "    print(answer, '를 추천해드릴게요')\n",
    "    \n",
    "    user_answer = input('어떤 장르의 웹툰을 추천해 드릴까요?')\n",
    "    answer, _ = predict_class(user_answer,gerne_model,gerne_model_data['words'],gerne_classes,gerne_words,stop_word=stop_word)\n",
    "    context += ' '+user_answer\n",
    "    gerne.append(answer)\n",
    "    print(answer, '를 추천해드릴게요')\n",
    "    \n",
    "    user_answer = input('어떤 스토리의 웹툰을 추천해 드릴까요?')\n",
    "    answer, _ = predict_class(user_answer,gerne_model,gerne_model_data['words'],gerne_classes,gerne_words,stop_word=stop_word)\n",
    "    gerne.append(answer)\n",
    "    print(answer, '를 추천해드릴게요')\n",
    "    \n",
    "    context += ' '+user_answer\n",
    "    data_set = add_user_sentence(context,gerne)\n",
    "    cosine_sim, idx_dict = cosine_similarity(data_set)\n",
    "    print(get_recommendations(data_set, idx_dict, cosine_sim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "어떤 콘텐츠를 추천해 드릴까요?웹툰\n",
      "['웹툰']\n",
      "webtoon 를 추천해드릴게요\n",
      "어떤 장르의 웹툰을 추천해 드릴까요?오늘 좀 우울해서 웃고싶어\n",
      "['오늘', '좀', '우울하다', '웃다']\n",
      "개그 를 추천해드릴게요\n",
      "어떤 스토리의 웹툰을 추천해 드릴까요?진짜 소리내서 웃을수 있는 웃기고 재밌는 웹툰\n",
      "['진짜', '소리내다', '웃다', '있다', '웃기', '고', '재밌다']\n",
      "개그 를 추천해드릴게요\n",
      "119개의 데이터셋과 1090개의 단어 구성\n",
      "26      네이버 앱피소드\n",
      "19     2019 병영일기\n",
      "115        슈퍼트리오\n",
      "41        내일은 웹툰\n",
      "87     쌉니다 천리마마트\n",
      "54          리얼주주\n",
      "47        이말년씨리즈\n",
      "18         오빠 왔다\n",
      "14          와탕카2\n",
      "92          진정친구\n",
      "Name: title, dtype: object\n"
     ]
    }
   ],
   "source": [
    "main_scenario()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
